{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steam Game  Engine Recommender System Part 1\n",
    "\n",
    "The first model of this project analyzes a dataset to predicts whether a randomly chosen user would play a randomly chosen game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import libraries and complete other pre-processing steps that will set up our data to input into a model\n",
    "\n",
    "2) Create neccessary dictionaries used for recommender system algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import scipy.spatial as sp\n",
    "import scipy.sparse\n",
    "\n",
    "li = []\n",
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u,g,d\n",
    "        \n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user,game,_ in readJSON(\"train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "c = 0\n",
    "X_train = []\n",
    "X_val = []\n",
    "userGames = defaultdict(set)\n",
    "gameUsers = defaultdict(set)\n",
    "userHours = defaultdict(list)\n",
    "gameHours = defaultdict(list)\n",
    "received_free = defaultdict(list)\n",
    "users = set()\n",
    "games = set()\n",
    "for user, game, d in readJSON('train.json.gz'):\n",
    "    if d['hours_transformed'] > 8.5:\n",
    "        continue\n",
    "    else:\n",
    "        userGames[user].add(game)\n",
    "        gameUsers[game].add(user)\n",
    "        users.add(user)\n",
    "        games.add(game)\n",
    "        userHours[user].append(d['hours_transformed'])\n",
    "        gameHours[game].append(d['hours_transformed'])\n",
    "        if 'compensation' in d.keys():\n",
    "            received_free[game].append(1)\n",
    "        else:\n",
    "            received_free[game].append(0)\n",
    "        c = np.random.choice([1, 0], p=[0.8, 0.2])\n",
    "        if c == 1:\n",
    "            X_train.append(d)\n",
    "        else:\n",
    "            X_val.append(d)\n",
    "    \n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train['y'] = np.array([1]*X_train.shape[0])\n",
    "X_val = pd.DataFrame(X_val)\n",
    "X_val['y'] = np.array([1]*X_val.shape[0])\n",
    "\n",
    "def user_didnt_play(uID):\n",
    "    g = list(userGames[uID])\n",
    "    g_prime = games.copy()\n",
    "    for i in g:\n",
    "        g_prime.remove(i)\n",
    "    return np.random.choice(list(g_prime))\n",
    "\n",
    "negative = []\n",
    "for i in list(X_train['userID']):\n",
    "    negative.append({'userID': i, 'gameID': user_didnt_play(i), 'y': 0})\n",
    "neg = pd.DataFrame(negative)\n",
    "X_train = X_train.append(neg).reset_index(drop=True)\n",
    "\n",
    "negative = []\n",
    "for i in list(X_val['userID']):\n",
    "    negative.append({'userID': i, 'gameID': user_didnt_play(i), 'y': 0})\n",
    "    \n",
    "neg = pd.DataFrame(negative)\n",
    "X_val = X_val.append(neg).reset_index(drop=True)\n",
    "\n",
    "X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
    "X_val = X_val.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train['hours_transformed'] = X_train['hours_transformed'].fillna(0)\n",
    "X_val['hours_transformed'] = X_val['hours_transformed'].fillna(0)\n",
    "X_train['compensation'] = X_train['compensation'].fillna(0)\n",
    "X_val['compensation'] = X_val['compensation'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define a Similarity function that will be used to asses the similarity between either two users or two games.\n",
    "\n",
    "4) Define a function that can take the data of a specific format and create a feature matrix (2D) that can be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/1.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "def make_features(data):\n",
    "    features = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print('Working on ' + str(i) + ' now...')\n",
    "        jac_vals = []\n",
    "        user, game = data['userID'][i], data['gameID'][i]\n",
    "        gUsers = gameUsers[game]\n",
    "        uGames = userGames[user]\n",
    "        \n",
    "        for x in uGames:\n",
    "            if x == game:\n",
    "                continue\n",
    "            jac_vals.append(Jaccard(gUsers, gameUsers[x]))\n",
    "                   \n",
    "        if len(jac_vals) == 0:\n",
    "            jac_max = 0\n",
    "        else:\n",
    "            jac_max = max(jac_vals)\n",
    "            \n",
    "            \n",
    "        pop_orNot = float(game in return1)\n",
    "        \n",
    "        if len(userHours[user]) == 0:\n",
    "            avg_userHours = 0\n",
    "        else:\n",
    "            avg_userHours = np.median(userHours[user])\n",
    "            \n",
    "        if len(gameHours[game]) == 0:\n",
    "            avg_gameHours = 0\n",
    "        else:\n",
    "            avg_gameHours = np.median(gameHours[game])\n",
    "        \n",
    "        if len(received_free[game]) == 0:\n",
    "            reFree = 0\n",
    "        else:\n",
    "            reFree = np.sum(received_free[user])\n",
    "\n",
    "        features.append([jac_max, pop_orNot, avg_userHours, reFree])\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Make the feature matrix based on our training data\n",
    "\n",
    "6) Use the LogisticRegression model from sklearn library to classify (0 or 1) whether a user would play a game\n",
    "\n",
    "7) Use the model to make predictions on our validation feature matrix.\n",
    "\n",
    "8) Assess the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0 now...\n",
      "Working on 1000 now...\n",
      "Working on 2000 now...\n",
      "Working on 3000 now...\n",
      "Working on 4000 now...\n",
      "Working on 5000 now...\n",
      "Working on 6000 now...\n",
      "Working on 7000 now...\n",
      "Working on 8000 now...\n",
      "Working on 9000 now...\n",
      "Working on 10000 now...\n",
      "Working on 11000 now...\n",
      "Working on 12000 now...\n",
      "Working on 13000 now...\n",
      "Working on 14000 now...\n",
      "Working on 15000 now...\n",
      "Working on 16000 now...\n",
      "Working on 17000 now...\n",
      "Working on 18000 now...\n",
      "Working on 19000 now...\n",
      "Working on 20000 now...\n",
      "Working on 21000 now...\n",
      "Working on 22000 now...\n",
      "Working on 23000 now...\n",
      "Working on 24000 now...\n",
      "Working on 25000 now...\n",
      "Working on 26000 now...\n",
      "Working on 27000 now...\n",
      "Working on 28000 now...\n",
      "Working on 29000 now...\n",
      "Working on 30000 now...\n",
      "Working on 31000 now...\n",
      "Working on 32000 now...\n",
      "Working on 33000 now...\n",
      "Working on 34000 now...\n",
      "Working on 35000 now...\n",
      "Working on 36000 now...\n",
      "Working on 37000 now...\n",
      "Working on 38000 now...\n",
      "Working on 39000 now...\n",
      "Working on 40000 now...\n",
      "Working on 41000 now...\n",
      "Working on 42000 now...\n",
      "Working on 43000 now...\n",
      "Working on 44000 now...\n",
      "Working on 45000 now...\n",
      "Working on 46000 now...\n",
      "Working on 47000 now...\n",
      "Working on 48000 now...\n",
      "Working on 49000 now...\n",
      "Working on 50000 now...\n",
      "Working on 51000 now...\n",
      "Working on 52000 now...\n",
      "Working on 53000 now...\n",
      "Working on 54000 now...\n",
      "Working on 55000 now...\n",
      "Working on 56000 now...\n",
      "Working on 57000 now...\n",
      "Working on 58000 now...\n",
      "Working on 59000 now...\n",
      "Working on 60000 now...\n",
      "Working on 61000 now...\n",
      "Working on 62000 now...\n",
      "Working on 63000 now...\n",
      "Working on 64000 now...\n",
      "Working on 65000 now...\n",
      "Working on 66000 now...\n",
      "Working on 67000 now...\n",
      "Working on 68000 now...\n",
      "Working on 69000 now...\n",
      "Working on 70000 now...\n",
      "Working on 71000 now...\n",
      "Working on 72000 now...\n",
      "Working on 73000 now...\n",
      "Working on 74000 now...\n",
      "Working on 75000 now...\n",
      "Working on 76000 now...\n",
      "Working on 77000 now...\n",
      "Working on 78000 now...\n",
      "Working on 79000 now...\n",
      "Working on 80000 now...\n",
      "Working on 81000 now...\n",
      "Working on 82000 now...\n",
      "Working on 83000 now...\n",
      "Working on 84000 now...\n",
      "Working on 85000 now...\n",
      "Working on 86000 now...\n",
      "Working on 87000 now...\n",
      "Working on 88000 now...\n",
      "Working on 89000 now...\n",
      "Working on 90000 now...\n",
      "Working on 91000 now...\n",
      "Working on 92000 now...\n",
      "Working on 93000 now...\n",
      "Working on 94000 now...\n",
      "Working on 95000 now...\n",
      "Working on 96000 now...\n",
      "Working on 97000 now...\n",
      "Working on 98000 now...\n",
      "Working on 99000 now...\n",
      "Working on 100000 now...\n",
      "Working on 101000 now...\n",
      "Working on 102000 now...\n",
      "Working on 103000 now...\n",
      "Working on 104000 now...\n",
      "Working on 105000 now...\n",
      "Working on 106000 now...\n",
      "Working on 107000 now...\n",
      "Working on 108000 now...\n",
      "Working on 109000 now...\n",
      "Working on 110000 now...\n",
      "Working on 111000 now...\n",
      "Working on 112000 now...\n",
      "Working on 113000 now...\n",
      "Working on 114000 now...\n",
      "Working on 115000 now...\n",
      "Working on 116000 now...\n",
      "Working on 117000 now...\n",
      "Working on 118000 now...\n",
      "Working on 119000 now...\n",
      "Working on 120000 now...\n",
      "Working on 121000 now...\n",
      "Working on 122000 now...\n",
      "Working on 123000 now...\n",
      "Working on 124000 now...\n",
      "Working on 125000 now...\n",
      "Working on 126000 now...\n",
      "Working on 127000 now...\n",
      "Working on 128000 now...\n",
      "Working on 129000 now...\n",
      "Working on 130000 now...\n",
      "Working on 131000 now...\n",
      "Working on 132000 now...\n",
      "Working on 133000 now...\n",
      "Working on 134000 now...\n",
      "Working on 135000 now...\n",
      "Working on 136000 now...\n",
      "Working on 137000 now...\n",
      "Working on 138000 now...\n",
      "Working on 139000 now...\n",
      "Working on 140000 now...\n",
      "Working on 141000 now...\n",
      "Working on 142000 now...\n",
      "Working on 143000 now...\n",
      "Working on 144000 now...\n",
      "Working on 145000 now...\n",
      "Working on 146000 now...\n",
      "Working on 147000 now...\n",
      "Working on 148000 now...\n",
      "Working on 149000 now...\n",
      "Working on 150000 now...\n",
      "Working on 151000 now...\n",
      "Working on 152000 now...\n",
      "Working on 153000 now...\n",
      "Working on 154000 now...\n",
      "Working on 155000 now...\n",
      "Working on 156000 now...\n",
      "Working on 157000 now...\n",
      "Working on 158000 now...\n",
      "Working on 159000 now...\n",
      "Working on 160000 now...\n",
      "Working on 161000 now...\n",
      "Working on 162000 now...\n",
      "Working on 163000 now...\n",
      "Working on 164000 now...\n",
      "Working on 165000 now...\n",
      "Working on 166000 now...\n",
      "Working on 167000 now...\n",
      "Working on 168000 now...\n",
      "Working on 169000 now...\n",
      "Working on 170000 now...\n",
      "Working on 171000 now...\n",
      "Working on 172000 now...\n",
      "Working on 173000 now...\n",
      "Working on 174000 now...\n",
      "Working on 175000 now...\n",
      "Working on 176000 now...\n",
      "Working on 177000 now...\n",
      "Working on 178000 now...\n",
      "Working on 179000 now...\n",
      "Working on 180000 now...\n",
      "Working on 181000 now...\n",
      "Working on 182000 now...\n",
      "Working on 183000 now...\n",
      "Working on 184000 now...\n",
      "Working on 185000 now...\n",
      "Working on 186000 now...\n",
      "Working on 187000 now...\n",
      "Working on 188000 now...\n",
      "Working on 189000 now...\n",
      "Working on 190000 now...\n",
      "Working on 191000 now...\n",
      "Working on 192000 now...\n",
      "Working on 193000 now...\n",
      "Working on 194000 now...\n",
      "Working on 195000 now...\n",
      "Working on 196000 now...\n",
      "Working on 197000 now...\n",
      "Working on 198000 now...\n",
      "Working on 199000 now...\n",
      "Working on 200000 now...\n",
      "Working on 201000 now...\n",
      "Working on 202000 now...\n",
      "Working on 203000 now...\n",
      "Working on 204000 now...\n",
      "Working on 205000 now...\n",
      "Working on 206000 now...\n",
      "Working on 207000 now...\n",
      "Working on 208000 now...\n",
      "Working on 209000 now...\n",
      "Working on 210000 now...\n",
      "Working on 211000 now...\n",
      "Working on 212000 now...\n",
      "Working on 213000 now...\n",
      "Working on 214000 now...\n",
      "Working on 215000 now...\n",
      "Working on 216000 now...\n",
      "Working on 217000 now...\n",
      "Working on 218000 now...\n",
      "Working on 219000 now...\n",
      "Working on 220000 now...\n",
      "Working on 221000 now...\n",
      "Working on 222000 now...\n",
      "Working on 223000 now...\n",
      "Working on 224000 now...\n",
      "Working on 225000 now...\n",
      "Working on 226000 now...\n",
      "Working on 227000 now...\n",
      "Working on 228000 now...\n",
      "Working on 229000 now...\n",
      "Working on 230000 now...\n",
      "Working on 231000 now...\n",
      "Working on 232000 now...\n",
      "Working on 233000 now...\n",
      "Working on 234000 now...\n",
      "Working on 235000 now...\n",
      "Working on 236000 now...\n",
      "Working on 237000 now...\n",
      "Working on 238000 now...\n",
      "Working on 239000 now...\n",
      "Working on 240000 now...\n",
      "Working on 241000 now...\n",
      "Working on 242000 now...\n",
      "Working on 243000 now...\n",
      "Working on 244000 now...\n",
      "Working on 245000 now...\n",
      "Working on 246000 now...\n",
      "Working on 247000 now...\n",
      "Working on 248000 now...\n",
      "Working on 249000 now...\n",
      "Working on 250000 now...\n",
      "Working on 251000 now...\n",
      "Working on 252000 now...\n",
      "Working on 253000 now...\n",
      "Working on 254000 now...\n",
      "Working on 255000 now...\n",
      "Working on 256000 now...\n",
      "Working on 257000 now...\n",
      "Working on 258000 now...\n",
      "Working on 259000 now...\n",
      "Working on 260000 now...\n",
      "Working on 261000 now...\n",
      "Working on 262000 now...\n",
      "Working on 263000 now...\n",
      "Working on 264000 now...\n",
      "Working on 265000 now...\n",
      "Working on 266000 now...\n",
      "Working on 267000 now...\n",
      "Working on 268000 now...\n",
      "Working on 269000 now...\n",
      "Working on 270000 now...\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "feats = make_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "model1 = LogisticRegression(C=10, fit_intercept=True)\n",
    "model1.fit(feats, X_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0 now...\n",
      "Working on 1000 now...\n",
      "Working on 2000 now...\n",
      "Working on 3000 now...\n",
      "Working on 4000 now...\n",
      "Working on 5000 now...\n",
      "Working on 6000 now...\n",
      "Working on 7000 now...\n",
      "Working on 8000 now...\n",
      "Working on 9000 now...\n",
      "Working on 10000 now...\n",
      "Working on 11000 now...\n",
      "Working on 12000 now...\n",
      "Working on 13000 now...\n",
      "Working on 14000 now...\n",
      "Working on 15000 now...\n",
      "Working on 16000 now...\n",
      "Working on 17000 now...\n",
      "Working on 18000 now...\n",
      "Working on 19000 now...\n",
      "Working on 20000 now...\n",
      "Working on 21000 now...\n",
      "Working on 22000 now...\n",
      "Working on 23000 now...\n",
      "Working on 24000 now...\n",
      "Working on 25000 now...\n",
      "Working on 26000 now...\n",
      "Working on 27000 now...\n",
      "Working on 28000 now...\n",
      "Working on 29000 now...\n",
      "Working on 30000 now...\n",
      "Working on 31000 now...\n",
      "Working on 32000 now...\n",
      "Working on 33000 now...\n",
      "Working on 34000 now...\n",
      "Working on 35000 now...\n",
      "Working on 36000 now...\n",
      "Working on 37000 now...\n",
      "Working on 38000 now...\n",
      "Working on 39000 now...\n",
      "Working on 40000 now...\n",
      "Working on 41000 now...\n",
      "Working on 42000 now...\n",
      "Working on 43000 now...\n",
      "Working on 44000 now...\n",
      "Working on 45000 now...\n",
      "Working on 46000 now...\n",
      "Working on 47000 now...\n",
      "Working on 48000 now...\n",
      "Working on 49000 now...\n",
      "Working on 50000 now...\n",
      "Working on 51000 now...\n",
      "Working on 52000 now...\n",
      "Working on 53000 now...\n",
      "Working on 54000 now...\n",
      "Working on 55000 now...\n",
      "Working on 56000 now...\n",
      "Working on 57000 now...\n",
      "Working on 58000 now...\n",
      "Working on 59000 now...\n",
      "Working on 60000 now...\n",
      "Working on 61000 now...\n",
      "Working on 62000 now...\n",
      "Working on 63000 now...\n",
      "Working on 64000 now...\n",
      "Working on 65000 now...\n",
      "Working on 66000 now...\n",
      "Working on 67000 now...\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\n",
    "preds = model1.predict(make_features(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7684007879656161"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8\n",
    "\n",
    "np.mean(np.array(preds) == X_val['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steam Game Engine NLP Part 2\n",
    "\n",
    "The second model of this project analyzes the dataset specifically the review of a game and predicts the genre of the game that was reviewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import libraries and complete other textual pre-processing steps that will set up our data to input into a model\n",
    "\n",
    "2) Obtain the counts of each word to use in our bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_Category.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a5a7db4b8648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_Category.json.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a5a7db4b8648>\u001b[0m in \u001b[0;36mreadGz\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_Category.json.gz'"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "        \n",
    "def readJSON(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        yield u,d\n",
    "        \n",
    "data = []\n",
    "\n",
    "for d in readGz(\"train_Category.json.gz\"):\n",
    "    data.append(d)\n",
    "\n",
    "Ntrain = (9*len(data))//10\n",
    "dataTrain = data[:Ntrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "sp = set(string.punctuation)\n",
    "wordCount = defaultdict(int)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    for w in tokens:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define a function that makes a one-hot encoded feature matrix out of our top NW most occuring words\n",
    "\n",
    "*Note:* Sparse matrix library used to optimize runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "NW = 3500\n",
    "words = [x[1] for x in counts[:NW]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    for w in tokens:\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "                \n",
    "    feat.append(1)\n",
    "    feat.append(len(tokens))\n",
    "    feat.append(float(datum['early_access']))\n",
    "    return feat\n",
    "\n",
    "nf = len(feature(data[0]))\n",
    "\n",
    "#Sparse matrix\n",
    "X = lil_matrix((len(data), nf))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if not (i % 1000):\n",
    "        print(i)\n",
    "    x = feature(data[i])\n",
    "    for j in range(nf):\n",
    "        if x[j]:\n",
    "            X[i,j] = x[j]\n",
    "\n",
    "y = [d['genreID'] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create training and validation datasets using a specific Ntrain value\n",
    "\n",
    "5) Use the LogisticRegression model from sklearn library to classify the genre of the game based on our derived feature matrix\n",
    "\n",
    "6) Use the model to make predictions on our validation feature matrix\n",
    "\n",
    "7) Assess the accuracy of the multi-class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xvalid = X[Ntrain:]\n",
    "yvalid = y[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "mod = linear_model.LogisticRegression(C=10, max_iter=10000)\n",
    "mod.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "pred = mod.predict(Xvalid)\n",
    "correct = pred == yvalid\n",
    "sum(correct) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Citations:\n",
    "\n",
    "Self-attentive sequential recommendation\n",
    "Wang-Cheng Kang, Julian McAuley\n",
    "ICDM, 2018\n",
    "\n",
    "Item recommendation on monotonic behavior chains\n",
    "Mengting Wan, Julian McAuley\n",
    "RecSys, 2018\n",
    "\n",
    "Generating and personalizing bundle recommendations on Steam\n",
    "Apurva Pathak, Kshitiz Gupta, Julian McAuley\n",
    "SIGIR, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
